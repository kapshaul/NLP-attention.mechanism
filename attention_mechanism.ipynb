{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9eb6_l6w2dm"
   },
   "source": [
    "Some initial installs of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1In4qUndEAx"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install datasets\n",
    "!mkdir examples\n",
    "!rm -rf sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukr89g5LKaCA"
   },
   "outputs": [],
   "source": [
    "# various torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "# our bleu score calculator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# utilities\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "#plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "#data and tokenization\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "#warning suppresion and logging\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    force=True)\n",
    "\n",
    "#grab torch device for later\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXrbJCwOryGy"
   },
   "source": [
    "Setting up our tokenizers, vocabularies, and PyTorch dataset classs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C_Ma6P4IPvxo"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "  def __init__(self, corpus, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word2idx, self.idx2word, self.freq = self.build_vocab(corpus)\n",
    "    self.size = len(self.word2idx)\n",
    "\n",
    "  def text2idx(self, text):\n",
    "    tokens = [str(x).strip().lower() for x in self.tokenizer(text)]\n",
    "    return [self.word2idx[t] if t in self.word2idx.keys() else self.word2idx['<UNK>'] for t in tokens]\n",
    "\n",
    "  def idx2text(self, idxs):\n",
    "    return [self.idx2word[i] if i in self.idx2word.keys() else '<UNK>' for i in idxs]\n",
    "\n",
    "\n",
    "  def build_vocab(self,corpus):\n",
    "    raw_tokens = [str(x).strip().lower() for x in self.tokenizer(\" \".join(corpus))]\n",
    "    cntr = Counter(raw_tokens)\n",
    "    freq = {t:c for t,c in cntr.items()}\n",
    "    tokens = [t for t,c in cntr.items() if c >= 2]\n",
    "    word2idx = {t:i+1 for i,t in enumerate(tokens)}\n",
    "    idx2word = {i+1:t for i,t in enumerate(tokens)}\n",
    "    word2idx['<UNK>'] = len(tokens)+1\n",
    "    idx2word[len(tokens)+1] = '<UNK>'\n",
    "    word2idx['<SOS>'] = len(tokens)+2\n",
    "    idx2word[len(tokens)+2] = '<SOS>'\n",
    "    word2idx['<EOS>'] = len(tokens)+3\n",
    "    idx2word[len(tokens)+3] = '<EOS>'\n",
    "    word2idx[''] = 0  #add padding token\n",
    "    idx2word[0] = ''\n",
    "\n",
    "    return word2idx, idx2word, freq\n",
    "\n",
    "class Multi30kDatasetEnDe(Dataset):\n",
    "\n",
    "  def __init__(self,split=\"train\", vocab_en = None, vocab_de = None):\n",
    "\n",
    "    dataset = load_dataset(\"bentrevett/multi30k\", split=split)\n",
    "    self.data_en = [x['en'] for x in dataset]\n",
    "    self.data_de = [x['de'] for x in dataset]\n",
    "\n",
    "    if vocab_en == None:\n",
    "      self.vocab_en = Vocabulary(self.data_en, spacy.load('en_core_web_sm').tokenizer)\n",
    "      self.vocab_de = Vocabulary(self.data_de, spacy.load('de_core_news_sm').tokenizer)\n",
    "    else:\n",
    "      self.vocab_en = vocab_en\n",
    "      self.vocab_de = vocab_de\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_en)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    numeralized_en = [self.vocab_en.word2idx['<SOS>']]+self.vocab_en.text2idx(self.data_en[idx])+[self.vocab_en.word2idx['<EOS>']]\n",
    "    numeralized_de = self.vocab_de.text2idx(self.data_de[idx])\n",
    "    return torch.tensor(numeralized_de),torch.tensor(numeralized_en)\n",
    "\n",
    "\n",
    "multi_train = Multi30kDatasetEnDe(split=\"train\")\n",
    "multi_val = Multi30kDatasetEnDe(split=\"validation\", vocab_en=multi_train.vocab_en, vocab_de=multi_train.vocab_de)\n",
    "multi_test = Multi30kDatasetEnDe(split=\"test\",  vocab_en=multi_train.vocab_en, vocab_de=multi_train.vocab_de)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4-WsVB5r9OX"
   },
   "source": [
    "Building out our dataloders with appropriate padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "md8luUZMPxLV"
   },
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "  xx = [ele[0] for ele in batch]\n",
    "  yy = [ele[1] for ele in batch]\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "B=128\n",
    "train_loader = DataLoader(multi_train, batch_size=B, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader = DataLoader(multi_val, batch_size=B, shuffle=False, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(multi_test, batch_size=B, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "src_vocab_size = multi_train.vocab_de.size+1\n",
    "dest_vocab_size = multi_train.vocab_en.size+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Kmvr62csBRU"
   },
   "source": [
    "Model definitions and utility functions for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "-2MaXivmwgxK"
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################################\n",
    "# Task 2.1\n",
    "##########################################################################################\n",
    "\n",
    "class SingleQueryScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    # kq_dim  is the  dimension  of keys  and  values. Linear  layers  should  be usedto  project  inputs  to these  dimensions.\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim, kq_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        #TODO\n",
    "        self.W_k = nn.Linear(enc_hid_dim * 2, kq_dim)\n",
    "        self.W_q = nn.Linear(dec_hid_dim, kq_dim)\n",
    "        self.kq_dim = kq_dim\n",
    "\n",
    "\n",
    "    #hidden  is h_t^{d} from Eq. (11)  and has  dim => [batch_size , dec_hid_dim]\n",
    "    #encoder_outputs  is the  word  representations  from Eq. (6)\n",
    "    # and has dim => [batch_size, src_len , enc_hid_dim * 2]\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # TODO\n",
    "        # Compute for q = W_q h_j^{e}\n",
    "        query = self.W_q(hidden).unsqueeze(1)\n",
    "        # Compute for k_t = W_k h_t^{d}\n",
    "        key = self.W_k(encoder_outputs)\n",
    "        # Compute for score = qk^T / sqrt(d)\n",
    "        score = torch.bmm(query, key.transpose(1, 2)).squeeze(1) / np.sqrt(self.kq_dim)\n",
    "        # Compute alpha = softmax(score)\n",
    "        alpha = F.softmax(score, dim=1)\n",
    "        # Compute a = sum(alpha_j v_j)\n",
    "        attended_val = torch.sum(encoder_outputs * alpha.unsqueeze(2), dim=1)\n",
    "\n",
    "        assert attended_val.shape == (hidden.shape[0], encoder_outputs.shape[2])\n",
    "        assert alpha.shape == (hidden.shape[0], encoder_outputs.shape[1])\n",
    "\n",
    "        return attended_val, alpha\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Model Definitions\n",
    "##########################################################################################\n",
    "\n",
    "class Dummy(nn.Module):\n",
    "\n",
    "    def __init__(self, dev):\n",
    "        super().__init__()\n",
    "        self.dev = dev\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        zout = torch.zeros( (hidden.shape[0], encoder_outputs.shape[2]) ).to(self.dev)\n",
    "        zatt = torch.zeros( (hidden.shape[0], encoder_outputs.shape[0]) ).to(self.dev)\n",
    "        return zout, zatt\n",
    "\n",
    "class MeanPool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        output = torch.mean(encoder_outputs, dim=1)\n",
    "        alpha = F.softmax(torch.ones(hidden.shape[0], encoder_outputs.shape[0]), dim=0)\n",
    "\n",
    "        return output, alpha\n",
    "\n",
    "class BidirectionalEncoder(nn.Module):\n",
    "    def __init__(self, src_vocab, emb_dim, enc_hid_dim, dec_hid_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hidden_dim = enc_hid_dim\n",
    "        self.emb = nn.Embedding(src_vocab, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, batch_first=True, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # embed source tokens\n",
    "        embedded = self.dropout(self.emb(src))\n",
    "\n",
    "        # process with bidirectional GRU model\n",
    "        enc_hidden_states, _ = self.rnn(embedded)\n",
    "\n",
    "        # compute a global sentence representation to feed as the initial hidden state of the decoder\n",
    "        # concatenate the forward GRU's representation after the last word and\n",
    "        # the backward GRU's representation after the first word\n",
    "\n",
    "        last_forward = enc_hidden_states[:, -1, :self.enc_hidden_dim]\n",
    "        first_backward = enc_hidden_states[:, 0, self.enc_hidden_dim:]\n",
    "\n",
    "        # transform to the size of the decoder hidden state with a fully-connected layer\n",
    "        sent = F.relu(self.fc(torch.cat((last_forward, first_backward), dim = 1)))\n",
    "\n",
    "\n",
    "\n",
    "        return enc_hidden_states, sent\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, attention, dropout=0.5,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, dec_hid_dim, batch_first=True)\n",
    "\n",
    "        self.fc_1 = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #Embed input\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        #Step decoder model forward\n",
    "        output, hidden = self.rnn(embedded.unsqueeze(1), hidden.unsqueeze(0))\n",
    "\n",
    "        #Perform attention operation\n",
    "        attended_feature, a = self.attention(hidden.squeeze(0), encoder_outputs)\n",
    "        #Make prediction\n",
    "        prediction = self.fc_out(torch.nn.functional.relu(self.dropout(self.fc_1(torch.cat((output.squeeze(1), attended_feature), dim = 1)))))\n",
    "\n",
    "        #Output prediction (scores for each word), the updated hidden state, and the attention map (for visualization)\n",
    "        return prediction, hidden.squeeze(0), a\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            # Step decoder model forward, getting output prediction, updated hidden, and attention distribution\n",
    "            output, hidden, a = self.decoder(trg[:,t-1], hidden, encoder_outputs)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[:,t,:] = output\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Train / Eval Functions\n",
    "##########################################################################################\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(desc=\"Epoch {}\".format(epoch), total=len(iterator), unit=\"batch\")\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(dev)\n",
    "        trg = batch[1].to(dev)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0].to(dev)\n",
    "            trg = batch[1].to(dev)\n",
    "\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Utility Functions\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def translate_sentence(sentence, vocab_en, vocab_de, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    numeralized_de = vocab_de.text2idx(sentence)\n",
    "    src_len = len(numeralized_de)\n",
    "\n",
    "\n",
    "    src_tensor = torch.tensor(numeralized_de).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "\n",
    "\n",
    "    trg_indexes = [vocab_en.word2idx['<SOS>']]\n",
    "\n",
    "    attentions = torch.zeros(max_len, 1, src_len).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "\n",
    "        attentions[i] = attention.squeeze()\n",
    "\n",
    "        pred_token = output.squeeze().argmax().item()\n",
    "\n",
    "\n",
    "\n",
    "        if pred_token == vocab_en.word2idx['<EOS>']:\n",
    "            break\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "    trg_tokens = [vocab_en.idx2word[i] for i in trg_indexes]\n",
    "    string = \"\"\n",
    "    for i in trg_indexes:\n",
    "        word = vocab_en.idx2word[i]\n",
    "        string += word + \" \"\n",
    "    print(string + \"\\n\")\n",
    "\n",
    "\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
    "\"\"\"\n",
    "def translate_sentence(sentence, vocab_en, vocab_de, model, device, max_len = 50, beams = 50):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    numeralized_de = vocab_de.text2idx(sentence)\n",
    "    src_len = len(numeralized_de)\n",
    "\n",
    "\n",
    "    src_tensor = torch.tensor(numeralized_de).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden_init = model.encoder(src_tensor)\n",
    "\n",
    "\n",
    "    trg_indexes = [vocab_en.word2idx['<SOS>']]\n",
    "\n",
    "    attentions_init = torch.zeros(max_len, 1, src_len).to(device)\n",
    "\n",
    "    beam = [(torch.tensor([trg_indexes[-1]]).to(device), 0, hidden_init, attentions_init)]\n",
    "    for i in range(max_len):\n",
    "        \n",
    "        candidates = []\n",
    "        for seq, score, hidden, attentions in beam:\n",
    "            # If the sequence already contains <EOS>, move it to candidates list \n",
    "            if seq[-1].item() == vocab_en.word2idx['<EOS>']:\n",
    "                candidates.append((seq, score, hidden, attentions))\n",
    "                continue\n",
    "            \n",
    "            # Get the last seqence\n",
    "            trg_tensor = torch.tensor([seq[-1]]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                output, hidden_b, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "            \n",
    "            # Log-Softmax to get log probability distribution\n",
    "            prob = F.log_softmax(output[-1], dim=-1).squeeze()\n",
    "            \n",
    "            # Get the top beam candidates\n",
    "            topk_prob, indices = torch.topk(prob, beams, dim=-1)\n",
    "\n",
    "            # Expansion stage to collect candidates\n",
    "            for b in range(beams):\n",
    "                next_token = indices[b]\n",
    "                next_prob = topk_prob[b]\n",
    "                new_seq = torch.cat((seq, next_token.unsqueeze(-1)), dim=0)                \n",
    "                new_score = score + next_prob\n",
    "                new_attentions = attentions.clone()\n",
    "                new_attentions[i] = attention.squeeze()\n",
    "                candidates.append((new_seq, new_score, hidden_b, new_attentions))\n",
    "\n",
    "        # Sort candidates by score\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Retain the top beams\n",
    "        beam = candidates[:beams]\n",
    "\n",
    "        # If all sequences in the beam are complete, exit the loop\n",
    "        if all(seq[-1].item() == vocab_en.word2idx['<EOS>'] for seq, _, _, _ in beam):\n",
    "            break\n",
    "\n",
    "    # Return the sequence with the highest log probability\n",
    "    best_seq, _, _, attentions = max(beam, key=lambda x: x[1])\n",
    "\n",
    "    trg_indexes = best_seq.cpu().numpy().tolist()\n",
    "\n",
    "    trg_tokens = [vocab_en.idx2word[i] for i in trg_indexes]\n",
    "    #text = \"\"\n",
    "    #for i in trg_indexes:\n",
    "    #    token = vocab_en.idx2word[i]\n",
    "    #    text += token + \" \"\n",
    "    #print(text + \"\\n\")\n",
    "\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
    "\n",
    "def save_attention_plot(sentence, translation, attention, vocab_de, index):\n",
    "\n",
    "    src = [str(x).strip().lower() for x in vocab_de.tokenizer(sentence)]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "    cax = ax.matshow(attention, cmap='Greys_r', vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.tick_params(labelsize=15)\n",
    "\n",
    "    x_ticks = [''] + src\n",
    "    y_ticks = [''] + translation\n",
    "\n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.savefig(\"examples/\"+str(index)+'_translation.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_bleu(test_data, model, device, max_len = 50):\n",
    "\n",
    "        trgs = []\n",
    "        pred_trgs = []\n",
    "\n",
    "        for src,trg in zip(test_data.data_de, test_data.data_en):\n",
    "\n",
    "\n",
    "            pred_trg, _ = translate_sentence(src, test_data.vocab_en, test_data.vocab_de, model, device, max_len)\n",
    "\n",
    "\n",
    "            #print(pred_trg)\n",
    "            pred_trgs.append(pred_trg)\n",
    "            trgs.append([[str(x).strip().lower() for x in test_data.vocab_en.tokenizer(trg)]])\n",
    "\n",
    "\n",
    "        return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbVL06r5ggOO"
   },
   "source": [
    "Model creation, training, and validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450,
     "referenced_widgets": [
      "921a52ee359b42778f6ec9fdd6c9c15a",
      "4dc76488320847bdbcc4928b8fdb9c3b",
      "4d23458777af497db07ff58280ea5f37",
      "ed77b4e7238945fe8ff68d2b1e37a676",
      "232e3e9f332a4afe9ced59cdd9a04526",
      "1727130f64fd487bb20861eeb817850f",
      "b076501448a34df7b2fe773947be1783",
      "2cef386530d340d8b1d234cd0c83a0df",
      "434503f8f0374465a27a905f9cd9472f",
      "75b718f6a6294f55b6501f9b8ceacf3f",
      "839120e5a5234d85a80e20a9e4528006"
     ]
    },
    "id": "4s2DTbEAfibv",
    "outputId": "648c270a-7e2a-4166-fba9-edbfadf37b54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:23:44 INFO     Training the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946a542e48ca4230a9d935beb9e98c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:24:18 INFO     Epoch: 01\tTrain Loss: 4.499 | Train PPL:  89.896\n",
      "2024-06-02 17:24:18 INFO     Epoch: 01\t Val. Loss: 3.486 |  Val. PPL:  32.667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2522c97dec40f79fe392062e59a274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:24:54 INFO     Epoch: 02\tTrain Loss: 3.475 | Train PPL:  32.284\n",
      "2024-06-02 17:24:54 INFO     Epoch: 02\t Val. Loss: 2.929 |  Val. PPL:  18.714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc14c0694144e0b9aa14d3d6d71fb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:25:31 INFO     Epoch: 03\tTrain Loss: 3.085 | Train PPL:  21.874\n",
      "2024-06-02 17:25:31 INFO     Epoch: 03\t Val. Loss: 2.670 |  Val. PPL:  14.439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdefa54784344643915a67c3024ea5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:26:06 INFO     Epoch: 04\tTrain Loss: 2.842 | Train PPL:  17.142\n",
      "2024-06-02 17:26:06 INFO     Epoch: 04\t Val. Loss: 2.521 |  Val. PPL:  12.447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14befca30f143adba42b4a93cc0c294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:26:42 INFO     Epoch: 05\tTrain Loss: 2.676 | Train PPL:  14.523\n",
      "2024-06-02 17:26:42 INFO     Epoch: 05\t Val. Loss: 2.413 |  Val. PPL:  11.169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473224447dce4577914e504d2a051379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:27:17 INFO     Epoch: 06\tTrain Loss: 2.545 | Train PPL:  12.739\n",
      "2024-06-02 17:27:17 INFO     Epoch: 06\t Val. Loss: 2.367 |  Val. PPL:  10.670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468252868ddb42979835b7a4f2648476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:27:44 INFO     Epoch: 07\tTrain Loss: 2.443 | Train PPL:  11.511\n",
      "2024-06-02 17:27:44 INFO     Epoch: 07\t Val. Loss: 2.326 |  Val. PPL:  10.241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd00163af134ba086215d46aacebae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:28:11 INFO     Epoch: 08\tTrain Loss: 2.359 | Train PPL:  10.580\n",
      "2024-06-02 17:28:11 INFO     Epoch: 08\t Val. Loss: 2.307 |  Val. PPL:  10.046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b20a61ec9f44d1c8d82d744402a2977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:28:47 INFO     Epoch: 09\tTrain Loss: 2.290 | Train PPL:   9.873\n",
      "2024-06-02 17:28:47 INFO     Epoch: 09\t Val. Loss: 2.284 |  Val. PPL:   9.815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc1364d1ce04f0ca825806627ebbfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/227 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:29:24 INFO     Epoch: 10\tTrain Loss: 2.229 | Train PPL:   9.287\n",
      "2024-06-02 17:29:24 INFO     Epoch: 10\t Val. Loss: 2.275 |  Val. PPL:   9.731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_embed_dim = 256\n",
    "hidden_dim = 512\n",
    "dropout_rate = 0.5\n",
    "\n",
    "##########################################################################################\n",
    "# Task 2.3\n",
    "##########################################################################################\n",
    "\n",
    "attn_type=\"sdp\"\n",
    "if attn_type == \"none\":\n",
    "    attn = Dummy(dev=dev)\n",
    "elif attn_type == \"mean\":\n",
    "    attn = MeanPool()\n",
    "elif attn_type == \"sdp\":\n",
    "    attn = SingleQueryScaledDotProductAttention(hidden_dim, hidden_dim)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "enc = BidirectionalEncoder(src_vocab_size, word_embed_dim, hidden_dim, hidden_dim, dropout_rate)\n",
    "dec = Decoder(dest_vocab_size, word_embed_dim, hidden_dim, hidden_dim, attn, dropout_rate)\n",
    "model = Seq2Seq(enc, dec, dev).to(dev)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "print(\"\\n\")\n",
    "logging.info(\"Training the model\")\n",
    "\n",
    "# Set up cross-entropy loss but ignore the pad token when computing it\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, epoch+1)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), attn_type+'-best-checkpoint.pt')\n",
    "\n",
    "    logging.info(f'Epoch: {epoch+1:02}\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    logging.info(f'Epoch: {epoch+1:02}\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSrDLzWfgTVU"
   },
   "source": [
    "Next bit of code loads our best checkpoint (based on validation loss) and does some test set evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "_ydLQfwe3SaF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 22:45:34 INFO     Running test evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 01:42:22 INFO     | Test Loss: 2.350 | Test PPL:  10.489 | Test BLEU 35.23\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(attn_type+'-best-checkpoint.pt'))\n",
    "\n",
    "print(\"\\n\")\n",
    "logging.info(\"Running test evaluation:\")\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "bleu = calculate_bleu(multi_test, model, dev)\n",
    "logging.info(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test BLEU {bleu*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpcDCnr1sVYy"
   },
   "source": [
    "Generate ten examples with visualized attention distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "y02YsRa9rrVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> two young men riding a very small cart full of a horse filled full of a horse . <EOS> \n",
      "\n",
      "\n",
      "---------654-----------\n",
      "src = Zwei junge Männer fahren auf einem sehr kleinen Wagen voller Kartoffeln, der von einem Pferd gezogen wird.\n",
      "trg = Two young men riding on a very small horse-drawn wagon full of potatoes.\n",
      "prd = two young men riding a very small cart full of a horse filled full of a horse . <EOS>\n",
      "<SOS> two indian men participate in a ceremony . <EOS> \n",
      "\n",
      "\n",
      "---------114-----------\n",
      "src = Zwei indische Männer nehmen an einer Zeremonie teil.\n",
      "trg = Two Indian men participating in a ceremony.\n",
      "prd = two indian men participate in a ceremony . <EOS>\n",
      "<SOS> a woman in a purple tank top and apron cleans a table with a sponge . <EOS> \n",
      "\n",
      "\n",
      "---------25-----------\n",
      "src = Eine Frau in einem pinken Pulli und einer Schürze putzt einen Tisch mit einem Schwamm.\n",
      "trg = A woman in a pink sweater and an apron, cleaning a table with a sponge.\n",
      "prd = a woman in a purple tank top and apron cleans a table with a sponge . <EOS>\n",
      "<SOS> a smiling man with a backpack in front of a boy with glasses in the air . <EOS> \n",
      "\n",
      "\n",
      "---------759-----------\n",
      "src = Ein lächelnder Mann mit Rucksack streckt vor einem Jungen mit Brille die Fäuste in die Luft.\n",
      "trg = A smiling man wearing a backpack holds his fists up in front of a boy in glasses.\n",
      "prd = a smiling man with a backpack in front of a boy with glasses in the air . <EOS>\n",
      "<SOS> a dog jumps to catch a ball while another watches . <EOS> \n",
      "\n",
      "\n",
      "---------281-----------\n",
      "src = Ein Hund springt um einen Ball zu fangen, während ein anderer zusieht.\n",
      "trg = One dog leaps to catch a softball while another looks on.\n",
      "prd = a dog jumps to catch a ball while another watches . <EOS>\n",
      "<SOS> a boy poses with a large green sign on his nose . <EOS> \n",
      "\n",
      "\n",
      "---------250-----------\n",
      "src = Ein Junge posiert mit einem großen grünen Insekt auf der Nase.\n",
      "trg = A boy poses with a large green insect on his nose.\n",
      "prd = a boy poses with a large green sign on his nose . <EOS>\n",
      "<SOS> many people are sitting around a tent outside . <EOS> \n",
      "\n",
      "\n",
      "---------228-----------\n",
      "src = Viele Menschen sitzen um ein Zelt im Freien.\n",
      "trg = Many people are sitting around a tent outside.\n",
      "prd = many people are sitting around a tent outside . <EOS>\n",
      "<SOS> a family is walking through a park . <EOS> \n",
      "\n",
      "\n",
      "---------142-----------\n",
      "src = Eine Familie spaziert durch einen Park.\n",
      "trg = A Family going for a walk in a park.\n",
      "prd = a family is walking through a park . <EOS>\n",
      "<SOS> these people are climbing the steps of the mountain . <EOS> \n",
      "\n",
      "\n",
      "---------754-----------\n",
      "src = Diese Personen klettern die Stufen zum Berg hoch\n",
      "trg = These people are climbing the steps to go the mountain\n",
      "prd = these people are climbing the steps of the mountain . <EOS>\n",
      "<SOS> a group of kids are climbing a good . <EOS> \n",
      "\n",
      "\n",
      "---------104-----------\n",
      "src = Eine Gruppe klettert bei kaltem Wetter.\n",
      "trg = A group of people are climbing in cold weather.\n",
      "prd = a group of kids are climbing a good . <EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "for i in range(10):\n",
    "    example_id = random.randint(0, len(multi_test))\n",
    "    src = multi_test.data_de[example_id]\n",
    "    trg = multi_test.data_en[example_id]\n",
    "\n",
    "    translation, attention = translate_sentence(src, multi_test.vocab_en, multi_test.vocab_de, model, dev)\n",
    "\n",
    "    print(f\"\\n---------{str(example_id)}-----------\")\n",
    "    print(f'src = {src}')\n",
    "    print(f'trg = {trg}')\n",
    "    print(f'prd = {\" \".join(translation)}')\n",
    "\n",
    "    save_attention_plot(src, translation, attention, multi_test.vocab_de, example_id)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1727130f64fd487bb20861eeb817850f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "232e3e9f332a4afe9ced59cdd9a04526": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cef386530d340d8b1d234cd0c83a0df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "434503f8f0374465a27a905f9cd9472f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4d23458777af497db07ff58280ea5f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cef386530d340d8b1d234cd0c83a0df",
      "max": 227,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_434503f8f0374465a27a905f9cd9472f",
      "value": 54
     }
    },
    "4dc76488320847bdbcc4928b8fdb9c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1727130f64fd487bb20861eeb817850f",
      "placeholder": "​",
      "style": "IPY_MODEL_b076501448a34df7b2fe773947be1783",
      "value": "Epoch 1:  24%"
     }
    },
    "75b718f6a6294f55b6501f9b8ceacf3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "839120e5a5234d85a80e20a9e4528006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "921a52ee359b42778f6ec9fdd6c9c15a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dc76488320847bdbcc4928b8fdb9c3b",
       "IPY_MODEL_4d23458777af497db07ff58280ea5f37",
       "IPY_MODEL_ed77b4e7238945fe8ff68d2b1e37a676"
      ],
      "layout": "IPY_MODEL_232e3e9f332a4afe9ced59cdd9a04526"
     }
    },
    "b076501448a34df7b2fe773947be1783": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed77b4e7238945fe8ff68d2b1e37a676": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75b718f6a6294f55b6501f9b8ceacf3f",
      "placeholder": "​",
      "style": "IPY_MODEL_839120e5a5234d85a80e20a9e4528006",
      "value": " 54/227 [06:37&lt;19:24,  6.73s/batch]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
